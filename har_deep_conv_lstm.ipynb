{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition Using Deep Convolutional Network + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a convolutional network + LSTM to do human activity recognition using the smart phone sensor data from the [WISDM](http://dl.acm.org/citation.cfm?doid=1964897.1964918) lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths\n",
    "\n",
    "We will define the project path so it will be easy to refer to files we need for building models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRJ_DIR = \"/home/har/har-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the conda environment so we can use the user-specificlibraries. The only way I know how to do this within the jupyter notebook is by inserting the environment paths in `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Keras](http://keras.io) library to build the deep learning model. It is a very straightforward wrapper around the popular tensor-based library [Theano](http://deeplearning.net/software/theano/introduction.html) and Google's [TensorFlow](https://www.tensorflow.org/). The user just need to connect the layers, the library will build the low-level parameters and operations by calling the backend libraries. Eventually the model is translated into C++ code for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Activation, BatchNormalization, Dense, Dropout, Flatten,\n",
    "    Conv2D, MaxPooling2D, LSTM, Permute, Reshape\n",
    ")\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, `deepsense` is a python library for processing HAR datasets. It also includes some scripts to train models. We will use the classes in the library to manipulate the [WISDM dataset](http://www.cis.fordham.edu/wisdm/dataset.php#actitracker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data set\n",
    "\n",
    "Most time we are spending when building a machine learning model is in two things:\n",
    "\n",
    "  1. Cleaning and consolidating data set\n",
    "  2. Tuning parameters\n",
    "  \n",
    "I have previously re-formatting the WISDM data set into training-ready format. Everything is wrapped within a [`pickle`](https://docs.python.org/2/library/pickle.html) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, meta, ACTIVITY_MAPPING = pickle.load(\n",
    "    open(os.path.join(PRJ_DIR, \"data/wisdm_subset.pkl\"), \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle file contains following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data matrices: `X` and `y`\n",
    "\n",
    "The transformed `numpy.ndarray` that can directly fed to the model\n",
    "\n",
    "  * `X` : the sensor data of size (`num_samples`, `window_size`, `num_channels`)\n",
    "  * `y` : the array of labeled activities for each window of size (`num_samples`)\n",
    "\n",
    "To transform the data from what you see in `wisdm.df` to `wisdm.X`, we first applied a resampling and denoising filter of frequency 20 Hz to the data, and then we applied a moving window of 4 seconds, with stride length 1 second to the signal. Therefore, the dimension of the training set is:\n",
    "\n",
    "  * `window_size = 80` (4 seconds * 20 Hz)\n",
    "  * `num_channels = 3` \n",
    "\n",
    "Another important aspect in the matrix transformation is that for convergence purpose, we transformed the input values to [-1, 1]. To do that, we divided the original acceleration measurement $(m/s^2)$ by $3g$, and capped off values that's $>1$ or $<-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1200, 80, 3)\n",
      "Shape of y: (1200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data: `meta`\n",
    "\n",
    "A data frame that connects the matrices back to the annotation, including the randomly split training and test set label (20% of test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>begin</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>r_std</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:21:52.350</td>\n",
       "      <td>695</td>\n",
       "      <td>2.145842</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Sitting</td>\n",
       "      <td>2013-07-03 17:10:37.800</td>\n",
       "      <td>1603</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2013-09-13 02:31:24.400</td>\n",
       "      <td>1799</td>\n",
       "      <td>2.753198</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>LyingDown</td>\n",
       "      <td>2013-06-04 17:31:15.650</td>\n",
       "      <td>1117</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-06 19:29:31.800</td>\n",
       "      <td>707</td>\n",
       "      <td>1.936306</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-06 18:06:42.800</td>\n",
       "      <td>626</td>\n",
       "      <td>1.650495</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-09-15 20:12:22.550</td>\n",
       "      <td>651</td>\n",
       "      <td>1.630012</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-13 18:10:51.900</td>\n",
       "      <td>668</td>\n",
       "      <td>2.183766</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-13 18:04:18.050</td>\n",
       "      <td>718</td>\n",
       "      <td>1.755982</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>Standing</td>\n",
       "      <td>2013-06-20 21:52:43.800</td>\n",
       "      <td>1603</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity                   begin  subject_id     r_std    set\n",
       "438     Walking 2008-01-10 21:21:52.350         695  2.145842  train\n",
       "630     Sitting 2013-07-03 17:10:37.800        1603  0.001763  train\n",
       "632     Walking 2013-09-13 02:31:24.400        1799  2.753198  train\n",
       "1153  LyingDown 2013-06-04 17:31:15.650        1117  0.002214   test\n",
       "899     Walking 2011-10-06 19:29:31.800         707  1.936306  train\n",
       "1077    Walking 2011-10-06 18:06:42.800         626  1.650495   test\n",
       "223     Walking 2011-09-15 20:12:22.550         651  1.630012  train\n",
       "97      Walking 2011-10-13 18:10:51.900         668  2.183766  train\n",
       "767     Walking 2011-10-13 18:04:18.050         718  1.755982  train\n",
       "603    Standing 2013-06-20 21:52:43.800        1603  0.015632  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity map: `ACTIVITY_MAPPING`\n",
    "\n",
    "The mapping between the code in `y` and the actual activity name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jogging': 5,\n",
       " 'LyingDown': 3,\n",
       " 'Sitting': 1,\n",
       " 'Stairs': 2,\n",
       " 'Standing': 0,\n",
       " 'Walking': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIVITY_MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data set\n",
    "\n",
    "We now split the data set into training and testing based on the index in the metadata. The testing set will not be used in learning weights in the network in any way. We will only use it to evaluate if our model is overfitting and select a stopping point for training.\n",
    "\n",
    "Note that we convert the matrix to float 32 for the benefit of running it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 1, ..., 0, 4, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = meta[meta[\"set\"] == \"train\"].index.values\n",
    "test_idx = meta[meta[\"set\"] == \"test\"].index.values\n",
    "\n",
    "X_train = X[train_idx].astype(np.float32)\n",
    "y_train = y[train_idx].astype(np.float32)\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that `y` is categorical, we also have to convert the vector of activity code into a matrix using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_classes = len(ACTIVITY_MAPPING) # number of activities \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes).astype(np.float32)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a deep learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "Let's first define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "nb_epoch = 10 # only train for 10 iteration\n",
    "num_channels = 3 # x, y, z\n",
    "sliding_window_length = 80 # 4-second window * 20 Hz sampling rate\n",
    "\n",
    "num_conv_filters = 32 # number of convolutional filters\n",
    "conv_filter_size = 3 # size of the convolutional filters\n",
    "num_lstm_units = 64 # number of LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sequential` model is the simplest way to connect layers: one after another. We will use it to build our network. Our goal is to build something like this:\n",
    "\n",
    "![](../figures/cnn_annot.png)\n",
    "\n",
    "This structure idea was taken from [this paper](https://www.ncbi.nlm.nih.gov/pubmed/26797612) (original source code [here](https://github.com/sussexwearlab/DeepConvLSTM), but it uses another wrapper library called [Lasagne](https://github.com/Lasagne/Lasagne)), where they use Conv + LSTM for gesture and locomotion recognition based on sensor data from various location in human body. Here I simplified it and added a few other layers to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 4 layers of conv\n",
    "model.add(Conv2D(filters=num_conv_filters, kernel_size=(conv_filter_size, 1),\n",
    "                input_shape=(sliding_window_length, num_channels, 1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(filters=num_conv_filters, kernel_size=(conv_filter_size, 1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Conv2D(filters=2*num_conv_filters, kernel_size=(conv_filter_size, 1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(filters=2*num_conv_filters, kernel_size=(conv_filter_size, 1)))\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Reshape((model.output_shape[1],\n",
    "                   model.output_shape[2] * model.output_shape[3])))\n",
    "\n",
    "# 2 layers of lstm\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(num_lstm_units, return_sequences=True))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(num_lstm_units))\n",
    "model.add(Dense(nb_classes,\n",
    "                kernel_regularizer=l1_l2(l1=1e-2, l2=1e-2),\n",
    "                activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the number of parameters and the shape of output in each layer by the `.summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 78, 3, 32)         128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 78, 3, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 78, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 76, 3, 32)         3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 76, 3, 32)         128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 76, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 38, 3, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 36, 3, 64)         6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 36, 3, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 36, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 34, 3, 64)         12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 34, 3, 64)         256       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 34, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 3, 64)         0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 17, 192)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 17, 192)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 17, 64)            65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 121,766\n",
      "Trainable params: 121,382\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "The convolutional layer has a sparse connection to the adjecent units:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/conv_1D_nn.png)\n",
    "\n",
    "In the figure above, lines with same color correspond to the same weight. \n",
    "\n",
    "For activation, the convolution layers use Rectified Linear Unit (ReLU), which is shown to have [better convergence](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) than `tanh` function.\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn1/relu.jpeg)\n",
    "\n",
    "You can see the first convolutional layer contains \n",
    "\n",
    "$$ 32 \\text{ feature maps (or convolutional units)} \\times 3 \\text{ (convolutional filter size)} + 32 \\text{ bias} = 128 \\text{ parameters}$$\n",
    "\n",
    "Similarly, the second layer contains \n",
    "\n",
    "$$ 32 \\times 32 \\times 3 + 32 = 3104 \\text{ parameters} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization layer\n",
    "\n",
    "According to [this paper](https://arxiv.org/pdf/1502.03167v3.pdf) from Google, a batch normalization layer can reduce internal covariate shift between layers and thus reduce the convergence time and might eliminatethe need for Dropout. Each unit in batch normailzation calculates the mean and standard deviation in the mini batch and shift the input by\n",
    "\n",
    "$$ y = \\gamma x + \\beta$$\n",
    "\n",
    "Therefore, the unmber of parameter is $2 \\times 32 = 64$ in the first BatchNormalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling layer\n",
    "\n",
    "A max pooling layer parttions input into non-overlapping rectangles and output the maximum in each subregion. It reduces the data size and thus speed up the computation. Usually the pooling size is 2, too much pooling may result in loss of too much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Long short-term memory (LSTM) layer is one of the most widely used recurrent neural network (RNN) layer. The units in the recurrent neural network forms directed cycles which allow them to exhibit temporal behavior. Therefore we often see the application of RNNs in sequential data such as [handwriting recognition](https://arxiv.org/pdf/1312.4569.pdf), [speech recognition](https://www.microsoft.com/en-us/research/publication/lstm-time-and-frequency-recurrence-for-automatic-speech-recognition/), or [sentimental analysis](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "A LSTM unit is different from other RNN unit in the sense that it has a forget gate:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/lstm_memorycell.png)\n",
    "\n",
    "For each of the unit at time $t$, it involves the following calculation:\n",
    "\n",
    "  * Input gate \n",
    "      $$i_t = \\sigma(W_ix_t + U_i h_{t-1} + b_i)$$\n",
    "  \n",
    "  * Candidate value for the state of memory cell \n",
    "      $$\\tilde{C}_t = tanh(W_cx_t + U_ch_{t-1} + b_c)$$\n",
    "  \n",
    "  * Activation of forget cell\n",
    "      $$f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)$$\n",
    "  \n",
    "  * The state of memory cell \n",
    "      $$C_t = i_t \\tilde{C}_t + f_t C_{t-1}$$\n",
    "  \n",
    "  * Output gate\n",
    "      $$o_t = \\sigma(W_ox_t + U_o h_{t-1} + b_o)$$\n",
    "  \n",
    "  * Output \n",
    "      $$h_t = o_t tanh(C_t)$$\n",
    "\n",
    "The above calculations involve the following parameters:\n",
    "  * $W$ : weight vector of `input_size`\n",
    "  * $U$ : weight vector of `output_size`\n",
    "  * $b$ : a scalar of bias\n",
    "\n",
    "Therefore, for instance, the first LSTM layer involves\n",
    "\n",
    "$(64 + 192 + 1) \\times 64 \\text{ units} \\times 4 (\\text{input gate, candidate state, forget, output gate}) = 65792$ parameters,\n",
    "\n",
    "and the second LSTM layer involves \n",
    "\n",
    "$(64 + 64 + 1) \\times 64 \\times 4 = 33024$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer\n",
    "\n",
    "Large number of parameters tends to overfit the training data, the number of parameters in deep neural network is gigantic. One simple technique that is offen used to prevent overfitting is to insert a [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) layer in between fully connected layers (_e.g._ LSTM). A dropout layer randomly set a fraction `p` of input units to 0 at each update during training time: \n",
    "\n",
    "![](../figures/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model for training\n",
    "\n",
    "With the model structure in place, we will configure the training methods for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsprop = RMSprop()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=rmsprop,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose optimizer\n",
    "\n",
    "Optimizers are basically a gradient descent algorithm used for optimizing the weights during each update. Here we will use the `RMSProp` optimizer as it is suggested to be [a good choice for recurrent neural networks](https://keras.io/optimizers/#rmsprop). A list of other optimizers and their formulation can be found in [this](http://sebastianruder.com/optimizing-gradient-descent/index.html)  great blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compile`\n",
    "\n",
    "Although the name might be misleading, this function does not really start compiling the model into C++ code, instead just setting more training behavior. Here we use the categorical [crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) as the target value to optimize. In each iteration (or as the deep learning guys like to call it, the `epoch`) we will also output the prediction accuracy as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... we start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but before that, since we are using a 2-D convolutional network here, we will have to reshape our input into `(num_samples, 1, sliding_window_length, num_channels)` to conform with the input shape of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, sliding_window_length, num_channels, 1)\n",
    "X_test = X_test.reshape(-1, sliding_window_length, num_channels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for time's sake. Let's just train on a subset of samples: 20,000 training, and 1,000 testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 8s - loss: 1.4476 - acc: 0.7030 - val_loss: 3.2243 - val_acc: 0.1450\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 7s - loss: 1.1085 - acc: 0.7740 - val_loss: 3.4092 - val_acc: 0.1450\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 8s - loss: 0.9920 - acc: 0.7800 - val_loss: 3.2407 - val_acc: 0.1450\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 8s - loss: 0.8774 - acc: 0.8010 - val_loss: 3.2606 - val_acc: 0.1100\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 7s - loss: 0.8099 - acc: 0.8160 - val_loss: 2.1421 - val_acc: 0.3650\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 9s - loss: 0.7745 - acc: 0.8040 - val_loss: 1.7441 - val_acc: 0.5550\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 7s - loss: 0.7190 - acc: 0.8170 - val_loss: 1.6148 - val_acc: 0.5300\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 7s - loss: 0.7171 - acc: 0.8110 - val_loss: 0.9086 - val_acc: 0.7300\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 7s - loss: 0.7057 - acc: 0.8210 - val_loss: 0.9771 - val_acc: 0.6900\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 8s - loss: 0.6987 - acc: 0.8180 - val_loss: 0.8692 - val_acc: 0.7400\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the validation loss was originally very high. At some point it starts to drop significantly, which is a characteristic when you use BatchNormalization layer (based on my personal experience). At 10th epoch, you already get around 80% validation accuracy.\n",
    "\n",
    "We can make predictions for new data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73999999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.apply_along_axis(lambda r: np.argmax(r), 1, Y_test_pred)\n",
    "\n",
    "# calculate the accuracy of the prediction\n",
    "np.mean(y_test_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the validation loss progression from the output from the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f56040cf310>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxGJQf8fGKHPhIYEh0wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f560406b190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "For other techniques for training model you may go through the following links:\n",
    "\n",
    "* Callback functions: [`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint), [`ReduceLROnPlateau`](https://keras.io/callbacks/#reducelronplateau), and [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) are quite useful\n",
    "* Hyperparameter search using `sklearn`'s [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) function. An example can be found in `kera`'s [example script](https://github.com/fchollet/keras/blob/master/examples/mnist_sklearn_wrapper.py).\n",
    "* Parallelization: [`mxnet`](https://github.com/dmlc/mxnet) or [Spark](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n",
    "* An nice overview on available deep learning library out there (more focused on python): [My Top 9 Favorite Python Deep Learning Libraries](http://www.pyimagesearch.com/2016/06/27/my-top-9-favorite-python-deep-learning-libraries/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
