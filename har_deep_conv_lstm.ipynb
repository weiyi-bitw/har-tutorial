{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Recognition Using Deep Convolutional Network + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a convolutional network + LSTM to do human activity recognition using the smart phone sensor data from the [WISDM](http://dl.acm.org/citation.cfm?doid=1964897.1964918) lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths\n",
    "\n",
    "First things first. We will define some paths so it will be easy to refer to files we need for building models. \n",
    "\n",
    "__NOTE__: The path setting is based on the basel HPC (hpclogin.bas.roche.com). You can change the paths according to your environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRJ_DIR = \"/pstore/home/chengw13/poc/dlexample\"\n",
    "CONDAENV_DIR = \"/pstore/data/predpoc/conda_env/dl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the conda environment so we can use the user-specificlibraries. The only way I know how to do this within the jupyter notebook is by inserting the environment paths in `sys.path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.join(CONDAENV_DIR, \"lib/python2.7/site-packages\"))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from Bio.Alphabet import IUPAC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [Keras](http://keras.io) library to build the deep learning model. It is a very straightforward wrapper around the popular tensor-based library [Theano](http://deeplearning.net/software/theano/introduction.html) and Google's [TensorFlow](https://www.tensorflow.org/). The user just need to connect the layers, the library will build the low-level parameters and operations by calling the backend libraries. Eventually the model is translated into C++ code for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Theano backend.\n",
    "\n",
    "If you are running these stuff for the first time, you should also execute the following command to setup the keras configuration. It by default uses Tensorflow backend, but the virtualenv does not have Tensorflow installed. We will use Theano backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this if it's the first time you run keras\n",
    "!cp ~/../chengw13/.keras/keras.json.theano ~/.keras/keras.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Activation, BatchNormalization, Dense, Dropout, Flatten,\n",
    "    Convolution2D, MaxPooling2D, LSTM, Permute, Reshape\n",
    ")\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l1l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, `deepsense` is a python library for processing HAR datasets. It also includes some scripts to train models. We will use the classes in the library to manipulate the [WISDM dataset](http://www.cis.fordham.edu/wisdm/dataset.php#actitracker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data set\n",
    "\n",
    "Most time we are spending when building a machine learning model is in two things:\n",
    "\n",
    "  1. Cleaning and consolidating data set\n",
    "  2. Tuning parameters\n",
    "  \n",
    "I have previously re-formatting the WISDM data set into training-ready format. Everything is wrapped within a [`pickle`](https://docs.python.org/2/library/pickle.html) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, X, y, meta, ACTIVITY_MAPPING = pickle.load(\n",
    "    open(os.path.join(PRJ_DIR, \"data/wisdm.pkl\"), \"rb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle file contains following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw data frame: `df`\n",
    "\n",
    "The original data frame read from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>activity_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2296664</th>\n",
       "      <td>726</td>\n",
       "      <td>LyingDown</td>\n",
       "      <td>2008-01-10 22:11:20.800</td>\n",
       "      <td>-9.003050</td>\n",
       "      <td>1.688923</td>\n",
       "      <td>3.364226</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784309</th>\n",
       "      <td>602</td>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:21:35.200</td>\n",
       "      <td>0.694638</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>-3.255263</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578351</th>\n",
       "      <td>607</td>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-06 17:45:16.727</td>\n",
       "      <td>0.612916</td>\n",
       "      <td>3.830723</td>\n",
       "      <td>3.371036</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537124</th>\n",
       "      <td>717</td>\n",
       "      <td>Jogging</td>\n",
       "      <td>2008-01-10 21:36:16.950</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>7.780000</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655528</th>\n",
       "      <td>1603</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2013-07-12 13:10:32.928</td>\n",
       "      <td>-0.008247</td>\n",
       "      <td>-0.011301</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101746</th>\n",
       "      <td>656</td>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-10-06 20:01:43.620</td>\n",
       "      <td>-0.812510</td>\n",
       "      <td>12.664864</td>\n",
       "      <td>3.097958</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656363</th>\n",
       "      <td>696</td>\n",
       "      <td>LyingDown</td>\n",
       "      <td>2008-01-10 21:49:18.450</td>\n",
       "      <td>2.492524</td>\n",
       "      <td>0.531194</td>\n",
       "      <td>9.806650</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465659</th>\n",
       "      <td>1512</td>\n",
       "      <td>Sitting</td>\n",
       "      <td>2013-03-22 12:18:00.362</td>\n",
       "      <td>-1.184970</td>\n",
       "      <td>3.323365</td>\n",
       "      <td>-9.030291</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924693</th>\n",
       "      <td>1696</td>\n",
       "      <td>Standing</td>\n",
       "      <td>2013-06-17 17:03:46.260</td>\n",
       "      <td>-0.049002</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533811</th>\n",
       "      <td>717</td>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:33:19.800</td>\n",
       "      <td>15.090000</td>\n",
       "      <td>18.770000</td>\n",
       "      <td>-6.090000</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id   activity               timestamp          x          y  \\\n",
       "2296664         726  LyingDown 2008-01-10 22:11:20.800  -9.003050   1.688923   \n",
       "1784309         602    Walking 2008-01-10 21:21:35.200   0.694638  -1.184970   \n",
       "1578351         607    Walking 2011-10-06 17:45:16.727   0.612916   3.830723   \n",
       "2537124         717    Jogging 2008-01-10 21:36:16.950   2.870000   0.840000   \n",
       "655528         1603    Sitting 2013-07-12 13:10:32.928  -0.008247  -0.011301   \n",
       "1101746         656    Walking 2011-10-06 20:01:43.620  -0.812510  12.664864   \n",
       "2656363         696  LyingDown 2008-01-10 21:49:18.450   2.492524   0.531194   \n",
       "2465659        1512    Sitting 2013-03-22 12:18:00.362  -1.184970   3.323365   \n",
       "924693         1696   Standing 2013-06-17 17:03:46.260  -0.049002   0.202400   \n",
       "2533811         717    Walking 2008-01-10 21:33:19.800  15.090000  18.770000   \n",
       "\n",
       "                z  activity_group  \n",
       "2296664  3.364226             548  \n",
       "1784309 -3.255263             118  \n",
       "1578351  3.371036             149  \n",
       "2537124  7.780000             526  \n",
       "655528   0.007025             733  \n",
       "1101746  3.097958             393  \n",
       "2656363  9.806650             489  \n",
       "2465659 -9.030291             697  \n",
       "924693  -0.008522             772  \n",
       "2533811 -6.090000             525  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data matrices: `X` and `y`\n",
    "\n",
    "The transformed `numpy.ndarray` that can directly fed to the model\n",
    "\n",
    "  * `X` : the sensor data of size (`num_samples`, `window_size`, `num_channels`)\n",
    "  * `y` : the array of labeled activities for each window of size (`num_samples`)\n",
    "\n",
    "To transform the data from what you see in `wisdm.df` to `wisdm.X`, we first applied a resampling and denoising filter of frequency 20 Hz to the data, and then we applied a moving window of 4 seconds, with stride length 1 second to the signal. Therefore, the dimension of the training set is:\n",
    "\n",
    "  * `window_size = 80` (4 seconds * 20 Hz)\n",
    "  * `num_channels = 3` \n",
    "\n",
    "Another important aspect in the matrix transformation is that for convergence purpose, we transformed the input values to [-1, 1]. To do that, we divided the original acceleration measurement $(m/s^2)$ by $3g$, and capped off values that's $>1$ or $<-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (142067, 80, 3)\n",
      "Shape of y: (142067,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of y: {}\".format(y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta data: `meta`\n",
    "\n",
    "A data frame that connects the matrices back to the annotation, including the randomly split training and test set label (20% of test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>begin</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:31:02.750</td>\n",
       "      <td>565</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102142</th>\n",
       "      <td>LyingDown</td>\n",
       "      <td>2013-07-02 05:29:37.300</td>\n",
       "      <td>1603</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110987</th>\n",
       "      <td>Sitting</td>\n",
       "      <td>2013-07-12 13:55:04.150</td>\n",
       "      <td>1603</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26358</th>\n",
       "      <td>Jogging</td>\n",
       "      <td>2008-01-10 21:32:03.200</td>\n",
       "      <td>608</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>Sitting</td>\n",
       "      <td>2008-01-10 21:38:42.250</td>\n",
       "      <td>565</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69362</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2011-09-15 19:50:57.250</td>\n",
       "      <td>706</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43832</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:35:32.150</td>\n",
       "      <td>646</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45595</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2008-01-10 21:40:31.700</td>\n",
       "      <td>648</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83969</th>\n",
       "      <td>Walking</td>\n",
       "      <td>2013-09-17 17:25:21.050</td>\n",
       "      <td>1104</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107548</th>\n",
       "      <td>Sitting</td>\n",
       "      <td>2013-07-12 12:57:45.150</td>\n",
       "      <td>1603</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         activity                   begin  subject_id    set\n",
       "4434      Walking 2008-01-10 21:31:02.750         565   test\n",
       "102142  LyingDown 2013-07-02 05:29:37.300        1603  train\n",
       "110987    Sitting 2013-07-12 13:55:04.150        1603  train\n",
       "26358     Jogging 2008-01-10 21:32:03.200         608  train\n",
       "4865      Sitting 2008-01-10 21:38:42.250         565  train\n",
       "69362     Walking 2011-09-15 19:50:57.250         706   test\n",
       "43832     Walking 2008-01-10 21:35:32.150         646   test\n",
       "45595     Walking 2008-01-10 21:40:31.700         648  train\n",
       "83969     Walking 2013-09-17 17:25:21.050        1104  train\n",
       "107548    Sitting 2013-07-12 12:57:45.150        1603  train"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity map: `ACTIVITY_MAPPING`\n",
    "\n",
    "The mapping between the code in `y` and the actual activity name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jogging': 5,\n",
       " 'LyingDown': 3,\n",
       " 'Sitting': 1,\n",
       " 'Stairs': 2,\n",
       " 'Standing': 0,\n",
       " 'Walking': 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACTIVITY_MAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data set\n",
    "\n",
    "We now split the data set into training and testing based on the index in the metadata. The testing set will not be used in learning weights in the network in any way. We will only use it to evaluate if our model is overfitting and select a stopping point for training.\n",
    "\n",
    "Note that we convert the matrix to float 32 for the benefit of running it on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_idx = meta[meta[\"set\"] == \"train\"].index.values\n",
    "test_idx = meta[meta[\"set\"] == \"test\"].index.values\n",
    "\n",
    "X_train = X[train_idx].astype(np.float32)\n",
    "y_train = y[train_idx].astype(np.float32)\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that `y` is categorical, we also have to convert the vector of activity code into a matrix using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(ACTIVITY_MAPPING) # number of activities \n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes).astype(np.float32)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a deep learning model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters\n",
    "\n",
    "Let's first define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "nb_epoch = 10 # only train for 10 iteration\n",
    "num_channels = 3 # x, y, z\n",
    "sliding_window_length = 80 # 4-second window * 20 Hz sampling rate\n",
    "\n",
    "num_conv_filters = 32 # number of convolutional filters\n",
    "conv_filter_size = 3 # size of the convolutional filters\n",
    "num_lstm_units = 64 # number of LSTM units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sequential` model is the simplest way to connect layers: one after another. We will use it to build our network. Our goal is to build something like this:\n",
    "\n",
    "![](../figures/cnn_annot.png)\n",
    "\n",
    "This structure idea was taken from [this paper](https://www.ncbi.nlm.nih.gov/pubmed/26797612) (original source code [here](https://github.com/sussexwearlab/DeepConvLSTM), but it uses another wrapper library called [Lasagne](https://github.com/Lasagne/Lasagne)), where they use Conv + LSTM for gesture and locomotion recognition based on sensor data from various location in human body. Here I simplified it and added a few other layers to speed up the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# 4 layers of conv\n",
    "model.add(Convolution2D(num_conv_filters, conv_filter_size, 1,\n",
    "                        input_shape=(1, sliding_window_length, num_channels),\n",
    "                        activation=\"relu\"))\n",
    "model.add(Convolution2D(num_conv_filters, conv_filter_size, 1,\n",
    "                        activation=\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "model.add(Convolution2D(num_conv_filters*2, conv_filter_size, 1,\n",
    "                        activation=\"relu\"))\n",
    "model.add(Convolution2D(num_conv_filters*2, conv_filter_size, 1,\n",
    "                        activation=\"relu\"))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "# reshape the model for LSTM\n",
    "model.add(Permute((2, 1, 3)))\n",
    "model.add(Reshape((model.output_shape[1],\n",
    "                   model.output_shape[2] * model.output_shape[3])))\n",
    "\n",
    "# 2 layers of lstm\n",
    "model.add(LSTM(num_lstm_units, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(num_lstm_units))\n",
    "model.add(Dense(nb_classes,\n",
    "                W_regularizer=l1l2(l1=1e-2, l2=1e-2),\n",
    "                activation=\"softmax\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the number of parameters and the shape of output in each layer by the `.summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 78, 3)     128         convolution2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 32, 76, 3)     3104        convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNormal(None, 32, 76, 3)     64          convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 38, 3)     0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 36, 3)     6208        maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 64, 34, 3)     12352       convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNormal(None, 64, 34, 3)     128         convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 17, 3)     0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 17, 64, 3)     0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 17, 192)       0           permute_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 17, 64)        65792       reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 17, 64)        0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 64)            33024       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 6)             390         lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 121190\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "\n",
    "The convolutional layer has a sparse connection to the adjecent units:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/conv_1D_nn.png)\n",
    "\n",
    "In the figure above, lines with same color correspond to the same weight. \n",
    "\n",
    "For activation, the convolution layers use Rectified Linear Unit (ReLU), which is shown to have [better convergence](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) than `tanh` function.\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn1/relu.jpeg)\n",
    "\n",
    "You can see the first convolutional layer contains \n",
    "\n",
    "$$ 32 \\text{ feature maps (or convolutional units)} \\times 3 \\text{ (convolutional filter size)} + 32 \\text{ bias} = 128 \\text{ parameters}$$\n",
    "\n",
    "Similarly, the second layer contains \n",
    "\n",
    "$$ 32 \\times 32 \\times 3 + 32 = 3104 \\text{ parameters} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization layer\n",
    "\n",
    "According to [this paper](https://arxiv.org/pdf/1502.03167v3.pdf) from Google, a batch normalization layer can reduce internal covariate shift between layers and thus reduce the convergence time and might eliminatethe need for Dropout. Each unit in batch normailzation calculates the mean and standard deviation in the mini batch and shift the input by\n",
    "\n",
    "$$ y = \\gamma x + \\beta$$\n",
    "\n",
    "Therefore, the unmber of parameter is $2 \\times 32 = 64$ in the first BatchNormalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling layer\n",
    "\n",
    "A max pooling layer parttions input into non-overlapping rectangles and output the maximum in each subregion. It reduces the data size and thus speed up the computation. Usually the pooling size is 2, too much pooling may result in loss of too much information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute layer\n",
    "\n",
    "Here we are perparing for reducing the dimension of the input layer to LSTM without misplacing the information. So we have to first reorder the dimension.\n",
    "\n",
    "A permute layer reorders the dimension of the input. For example, for an input of `(num_samples, num_rows, num_cols)`, a permute layer of `Permute((2, 1))` would be a transpose the input into `(num_samples, num_cols, num_rows)`. Here we do a `Permute((2, 1, 3))` to a input of `(num_conv_filters, time_points, num_channels)` to make it into `(time_points, num_conv_filters, num_channels)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape layer\n",
    "\n",
    "As the name suggsted, it reshape the input. The total elements in the output must be the same as the input. The logic of merging dimensions follows the logic of that in `numpy`.\n",
    "\n",
    "Here we reshape the input from `(time_points, num_conv_filters, num_channels)` into `(time_points, num_conv_filters * num_channels)`, where we mixed the channels but keep the time information for LSTM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Long short-term memory (LSTM) layer is one of the most widely used recurrent neural network (RNN) layer. The units in the recurrent neural network forms directed cycles which allow them to exhibit temporal behavior. Therefore we often see the application of RNNs in sequential data such as [handwriting recognition](https://arxiv.org/pdf/1312.4569.pdf), [speech recognition](https://www.microsoft.com/en-us/research/publication/lstm-time-and-frequency-recurrence-for-automatic-speech-recognition/), or [sentimental analysis](http://deeplearning.net/tutorial/lstm.html).\n",
    "\n",
    "A LSTM unit is different from other RNN unit in the sense that it has a forget gate:\n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/lstm_memorycell.png)\n",
    "\n",
    "For each of the unit at time $t$, it involves the following calculation:\n",
    "\n",
    "  * Input gate \n",
    "      $$i_t = \\sigma(W_ix_t + U_i h_{t-1} + b_i)$$\n",
    "  \n",
    "  * Candidate value for the state of memory cell \n",
    "      $$\\tilde{C}_t = tanh(W_cx_t + U_ch_{t-1} + b_c)$$\n",
    "  \n",
    "  * Activation of forget cell\n",
    "      $$f_t = \\sigma(W_fx_t + U_fh_{t-1} + b_f)$$\n",
    "  \n",
    "  * The state of memory cell \n",
    "      $$C_t = i_t \\tilde{C}_t + f_t C_{t-1}$$\n",
    "  \n",
    "  * Output gate\n",
    "      $$o_t = \\sigma(W_ox_t + U_o h_{t-1} + b_o)$$\n",
    "  \n",
    "  * Output \n",
    "      $$h_t = o_t tanh(C_t)$$\n",
    "\n",
    "The above calculations involve the following parameters:\n",
    "  * $W$ : weight vector of `input_size`\n",
    "  * $U$ : weight vector of `output_size`\n",
    "  * $b$ : a scalar of bias\n",
    "\n",
    "Therefore, for instance, the first LSTM layer involves\n",
    "\n",
    "$(64 + 192 + 1) \\times 64 \\text{ units} \\times 4 (\\text{input gate, candidate state, forget, output gate}) = 65792$ parameters,\n",
    "\n",
    "and the second LSTM layer involves \n",
    "\n",
    "$(64 + 64 + 1) \\times 64 \\times 4 = 33024$ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout layer\n",
    "\n",
    "Large number of parameters tends to overfit the training data, the number of parameters in deep neural network is gigantic. One simple technique that is offen used to prevent overfitting is to insert a [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) layer in between fully connected layers (_e.g._ LSTM). A dropout layer randomly set a fraction `p` of input units to 0 at each update during training time: \n",
    "\n",
    "![](../figures/dropout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure model for training\n",
    "\n",
    "With the model structure in place, we will configure the training methods for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = RMSprop()\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=rmsprop,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose optimizer\n",
    "\n",
    "Optimizers are basically a gradient descent algorithm used for optimizing the weights during each update. Here we will use the `RMSProp` optimizer as it is suggested to be [a good choice for recurrent neural networks](https://keras.io/optimizers/#rmsprop). A list of other optimizers and their formulation can be found in [this](http://sebastianruder.com/optimizing-gradient-descent/index.html)  great blog post. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `compile`\n",
    "\n",
    "Although the name might be misleading, this function does not really start compiling the model into C++ code, instead just setting more training behavior. Here we use the categorical [crossentropy](https://en.wikipedia.org/wiki/Cross_entropy) as the target value to optimize. In each iteration (or as the deep learning guys like to call it, the `epoch`) we will also output the prediction accuracy as output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally ... we start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but before that, since we are using a 2-D convolutional network here, we will have to reshape our input into `(num_samples, 1, sliding_window_length, num_channels)` to conform with the input shape of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, sliding_window_length, num_channels)\n",
    "X_test = X_test.reshape(-1, 1, sliding_window_length, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for time's sake. Let's just train on a subset of samples: 20,000 training, and 1,000 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = np.random.choice(np.arange(X_train.shape[0]),\n",
    "                             20000, replace=False)\n",
    "idx_test = np.random.choice(np.arange(X_test.shape[0]),\n",
    "                            1000, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 40s - loss: 1.0645 - acc: 0.7863 - val_loss: 2.9199 - val_acc: 0.2350\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.7182 - acc: 0.8267 - val_loss: 2.9416 - val_acc: 0.2350\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.6506 - acc: 0.8320 - val_loss: 3.0161 - val_acc: 0.2330\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.6212 - acc: 0.8388 - val_loss: 2.6957 - val_acc: 0.2360\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5977 - acc: 0.8453 - val_loss: 1.1394 - val_acc: 0.6300\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5727 - acc: 0.8520 - val_loss: 0.7244 - val_acc: 0.7830\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5516 - acc: 0.8632 - val_loss: 0.7514 - val_acc: 0.7760\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5363 - acc: 0.8656 - val_loss: 0.6359 - val_acc: 0.7790\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5251 - acc: 0.8700 - val_loss: 0.8809 - val_acc: 0.7180\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 40s - loss: 0.5177 - acc: 0.8711 - val_loss: 0.6455 - val_acc: 0.7940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[idx_train], Y_train[idx_train],\n",
    "                    batch_size=batch_size,\n",
    "                    nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test[idx_test], Y_test[idx_test]),\n",
    "                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the validation loss was originally very high. At some point it starts to drop significantly, which is a characteristic when you use BatchNormalization layer (based on my personal experience). At 10th epoch, you already get around 80% validation accuracy.\n",
    "\n",
    "We can make predictions for new data by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79632562559391828"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "y_test_pred = np.apply_along_axis(lambda r: np.argmax(r), 1, Y_test_pred)\n",
    "\n",
    "# calculate the accuracy of the prediction\n",
    "np.mean(y_test_pred == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the validation loss progression from the output from the `fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fff665fc150>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHBJJREFUeJzt3XuclOV5//HPtSyCgoISToocFAgiCJ4ABXXszyQgBgvE\nGuOrttZ4NlqT2IghhWpstKlGjUmVahI1Ek00EI8EqFmJRFFBBAUUzwgu8VdAA6jhcPWPe1bWZZaZ\nZWfmfubZ7/v1mtfO7Dw782WBa+655r7vx9wdERGpfFWxA4iISHGooIuIpIQKuohISqigi4ikhAq6\niEhKqKCLiKRE3oJuZm3MbIGZvWBmS81sSo5jTjCzDWa2KHuZXJq4IiLSmOp8B7j7J2Z2ortvNrNW\nwHwze9zdn21w6Dx3H1eamCIikk9BLRd335y92obwIpBrNZIVK5SIiDRdQQXdzKrM7AWgFpjj7s/l\nOOwYM1tsZo+a2cCiphQRkbwKHaFvd/fDgR7A8BwFeyHQ092HArcCM4sbU0RE8rGm7uViZt8DNrn7\njbs45k3gSHdf1+D72jhGRGQ3uHvetnYhs1w+Z2Ydstf3BL4ArGhwTNd614cRXig+U8zrhUrUZcqU\nKdEzVEouZVKmlpAriZkKlXeWC9AduMvMqggvAPe7+2Nmdn6ozz4N+IqZXQhsAT4CTi84gYiIFEUh\n0xaXAkfk+P7t9a7/BPhJcaOJiEhTtPiVoplMJnaEnJKYS5kKo0yFS2KuJGYqVJM/FG3Wk5l5OZ9P\nRCQNzAwvxoeiIiJSGVTQRURSQgVdRCQlVNBFRFJCBV1EJCVU0EVEUkIFXUQkJVTQRURSQgVdRCQl\nVNBFRFJCBV1EJCVU0EVEUkIFXUQkJVTQRURSQgVdRCQlVNBFRFJCBV1EJCVU0EVEUkIFXUQkJVTQ\nRURSQgVdRCQlVNBFRFJCBV1EJCVU0EVEUkIFXUQkJVTQRURSIm9BN7M2ZrbAzF4ws6VmNqWR424x\ns5VmttjMhhY/qoiI7Ep1vgPc/RMzO9HdN5tZK2C+mT3u7s/WHWNmY4CD3b2fmQ0HbgNGlC62iIg0\nVFDLxd03Z6+2IbwIeINDTgXuzh67AOhgZl2LFVJERPLLO0IHMLMqYCFwMPATd3+uwSEHAKvq3V6d\n/d7aYoRMqy1b4K234NVXP3tZvRq+8Q248EKo0qccIlKgggq6u28HDjezfYCZZjbQ3ZeVNlo6uMN7\n7+0o1q+8suP622/D/vtD//7hMmgQTJgA7drBFVfA9Onw3/8NAwfG/lOISCUoqKDXcfcPzewPwGig\nfkFfDRxY73aP7Pd2MnXq1E+vZzIZMplMUyIk1oYNO4+0X30VVq6EvfYKBfvznw9fjzsufD3oIGjb\nNvfj/fGPcNttcPzxYbR+5ZXQpk15/0wiEkdNTQ01NTVN/jlzb9gOb3CA2eeALe7+gZntCfweuM7d\nH6t3zMnAxe4+1sxGADe5+04fipqZ53u+JPv4Y3j99dyFe9OmHSPtusLdvz/06wcdO+7+c65aBRdd\nBG+8AXfcAcccU7w/j4hUBjPD3S3vcQUU9MHAXYQPUKuA+939WjM7H3B3n5Y97lbCyH0TcLa7L8rx\nWIkv6Nu2hSKaq2ivWQO9eu1ctPv3h+7dwfL+unePO/zmN3DZZXDaaXDttbD33qV5LhFJnqIV9GKK\nUdC3bAmj58Yu69fDa6/t6G+//jp06pS7aPfuDa1blzX+Z6xbB9/+NvzP/8BPfwpjx8bLIiLlU1EF\nfevWUFw3bmy88O7ufVu3hg8Z61/at99xvUMH6Nt3R9Hu2zfcn2Rz58L558Pw4XDTTdClS+xEIlJK\niS3oRxzhjRbd+oU21yXf/bmOadOmdK2QmDZvhqlT4a674Ic/hL//+3T+OUUkwQX9ued8pyKc1qJb\nDgsXwte/Dp07w+23Q58+sROJSLEltqAn/UPRSrRlC9x4YxipX3UVXHopVDdpQqqIJJkKegu0ciWc\nd174TOGOO2DIkNiJRKQYCi3oWlieIv36wRNPwAUXwBe+AN/9bpg7LyItgwp6ypjBOefAiy+GqZhD\nhsCTT8ZOJSLloJZLys2cCZdcEuasX39981atikgcarkIAH/7t/Dyy2HXxkGDYMaM2IlEpFQ0Qm9B\n/vhHOPdcOPRQ+PGPw06PIpJ8GqHLTo47DhYvDtvxDhkStubV66tIemiE3kItXRoWJO25J0ybFrY9\nEJFk0ghddmnwYPjTn2D8eDj2WPjBD8ICJRGpXBqhC2+9Feau19aGBUlHHRU7kYjUpxG6FKx3b3j8\n8XDau1NOCVv0btoUO5WINJUKugBhQdKZZ4be+tq1oSUzZ07sVCLSFGq5SE6zZoU2TCYDN9wQTvoh\nInGo5SLNMno0vPRSWFk6aBDcd5+mOIoknUbokteCBaEdc9118JWvxE4j0vJohC5FM3w4TJoEv/51\n7CQisisaoUtB3n8/nG+1tjYsRhKR8tEIXYqqc2c44giYPTt2EhFpjAq6FGziRPjtb2OnEJHGqOUi\nBXv33bCpV20ttG4dO41Iy6GWixRdjx7hNHc1NbGTiEguKujSJBMmwIMPxk4hIrmo5SJN8tprMGoU\nrF4NrVrFTiPSMqjlIiXRty907QpPPx07iYg0pIIuTTZhgma7iCRR3oJuZj3M7Akze9nMlprZpTmO\nOcHMNpjZouxlcmniShLUFXR1z0SSpbqAY7YC33T3xWbWHlhoZrPdfUWD4+a5+7jiR5SkGTQoTFtc\ntAiOPDJ2GhGpk3eE7u617r44e30jsBw4IMeheRv2kg5mWmQkkkRN6qGbWW9gKLAgx93HmNliM3vU\nzAYWIZskmProIslTSMsFgGy75QHgsuxIvb6FQE9332xmY4CZQM7zyE+dOvXT65lMhkwm08TIkgRH\nHQUbN8KyZTBQL98iRVVTU0PNbqzgK2geuplVA48Aj7v7zQUc/yZwpLuva/B9zUNPkUsvhS5dYLI+\nAhcpqWLPQ/8ZsKyxYm5mXetdH0Z4oViX61hJD/XRRZIl7wjdzEYC84ClgGcvVwG9AHf3aWZ2MXAh\nsAX4CLjc3Xfqs2uEni7btkH37uGMRn36xE4jkl6FjtC19F+a5dxzYcAA+Na3YicRSS8t/Zey0GwX\nkeTQCF2a5a9/DXu7LFsW2i8iUnwaoUtZ7LEHjB0LM2fGTiIiKujSbNojXSQZ1HKRZtu0CfbfH954\nAzp1ip1GJH3UcpGyadcOTjoJHn44dhKRlk0FXYpCs11E4lPLRYpiwwbo2TOcmm7vvWOnEUkXtVyk\nrDp2hJEj4bHHYicRablU0KVotLeLSFxquUjR/PnP0L8/1NZC27ax04ikh1ouUnZdusDQoTBnTuwk\nIi2TCroUlRYZicSjlosU1apVYZReWxtOJC0izaeWi0Rx4IHQty88+WTsJCItjwq6FJ0WGYnEoZaL\nFN3KlXD88WGRUZWGDCLNppaLRNOvH3TuDE8/HTuJSMuigi4lobaLSPmpoEtJ1BV0ddhEykcFXUpi\n8GBo1QpeeCF2EpGWQwVdSsJMe7uIlJsKupSM+ugi5aWCLiVz9NHw4YewfHnsJCItgwq6lExVFYwf\nr1G6SLmooEtJqY8uUj4q6FJSo0bBO+/AW2/FTiKSfiroUlLV1XDqqTBjRuwkIumXt6CbWQ8ze8LM\nXjazpWZ2aSPH3WJmK81ssZkNLX5UqVTaI12kPPJuzmVm3YBu7r7YzNoDC4FT3X1FvWPGAJe4+1gz\nGw7c7O4jcjyWNudqgT75BLp1g2XLoHv32GlEKk/RNudy91p3X5y9vhFYDhzQ4LBTgbuzxywAOphZ\n1yanllRq0wZOPhl+97vYSUTSrUk9dDPrDQwFFjS46wBgVb3bq9m56EsLpkVGIqVXXeiB2XbLA8Bl\n2ZH6bpk6deqn1zOZDJlMZncfSirI6NHwT/8E69bBfvvFTiOSbDU1NdTU1DT55wo6wYWZVQOPAI+7\n+8057r8N+IO735+9vQI4wd3XNjhOPfQWbMKEMOPlH/4hdhKRylLsE1z8DFiWq5hnPQSclX3iEcCG\nhsVcRG0XkdIqZJbLSGAesBTw7OUqoBfg7j4te9ytwGhgE3C2uy/K8VgaobdgGzZAz56wZg20bx87\njUjlKHSErnOKSlmNHg3nnAOnnRY7iUjl0DlFJZEmTtQiI5FS0QhdyurPf4b+/aG2Ftq2jZ1GpDJo\nhC6J1KULDBkCc+fGTiKSPiroUnaa7SJSGmq5SNmtWgWHHw7vvQetW8dOI5J8arlIYh14IBx0EMyb\nFzuJSLqooEsUaruIFJ9aLhLFq69CJgPvvhvOPSoijVPLRRKtf/+wSdczz8ROIpIeKugSjU4gLVJc\nKugSTV0fXV04keJQQZdoDjsMzODFF2MnEUkHFXSJxkwnkBYpJhV0iUp9dJHiUUGXqIYNC/ukr1gR\nO4lI5VNBl6iqqmD8eJgxI3YSkcqngi7RqY8uUhxaKSrRbd0K3bvD889Dr16x04gkj1aKSsWoroZx\n49R2EWkuFXRJBG3WJdJ8arlIInzyCXTrFma7dO0aO41IsqjlIhWlTRsYMwZmzoydRKRyqaBLYqjt\nItI8arlIYmzcCPvvD2+/DfvuGzuNSHKo5SIVp317+Ju/gUceiZ1EpDKpoEuiTJyoRUYiu0stF0mU\n9evD4qI1a8KIXUTUcpEKte++cMwxMGtW7CQilSdvQTezO81srZktaeT+E8xsg5ktyl4mFz+mtCSa\n7SKye/K2XMxsFLARuNvdD8tx/wnAt9x9XN4nU8tFCrB2LQwYALW1YX66SEtXtJaLuz8FrM/3fIUG\nE8mna1cYPBjmzo2dRKSyFKuHfoyZLTazR81sYJEeU1owtV1Emq66CI+xEOjp7pvNbAwwE+jf2MFT\np0799HomkyGTyRQhgqTN+PFw7bVha93qYvwrFakgNTU11NTUNPnnCpq2aGa9gIdz9dBzHPsmcKS7\nr8txn3roUrCjj4brrw+LjURasmJPWzQa6ZObWdd614cRXiR2KuYiTaW2i0jTFDLLZTqQAToBa4Ep\nwB6Au/s0M7sYuBDYAnwEXO7uCxp5LI3QpWCvvBJG56tWhXOPirRUhY7QtVJUEu3QQ+HOO2HEiNhJ\nROLRSlFJBe3tIlI4FXRJtLo+ut7YieSngi6JNmRIKOZLcm48ISL1qaBLoplptotIoVTQJfHURxcp\njAq6JN7w4bBuXZjGKCKNU0GXxKuqClsBzJgRO4lIsqmgS0VQH10kPy0skoqwdSt06waLFkHPnrHT\niJSXFhZJqlRXw7hxaruI7IoKulQMtV1Edk0tF6kYH38c2i6vvBLOaiTSUqjlIqnTti2MGQMPPRQ7\niUgyqaBLRZkwQYuMRBqjlotUlI0bYf/94Z13oGPH2GlEykMtF0ml9u3hxBPhkUdiJxFJHhV0qTgT\nJ2q2i0guarlIxVm3Dvr0gTVroF272GlESk8tF0mt/fYLG3bNmhU7iUiyqKBLRdIiI5GdqeUiFam2\nFg45JHxt0yZ2GpHSUstFUq1bNxg0CObOjZ1EJDlU0KViXXwxnH8+PPts7CQiyaCCLhXrq1+Fn/4U\nxo6FX/0qdhqR+NRDl4q3ZAmceiqceSZcfXU4w5FImhTaQ1dBl1R4//0w86VzZ7j77rCiVCQt9KGo\ntCidO4cPSDt2hFGjwl4vIi2NCrqkRps2cOedcNZZMGIEPP107EQi5ZW3oJvZnWa21syW7OKYW8xs\npZktNrOhxY0oUjgz+OY34Y47Ql/97rtjJxIpn0JG6D8HvtTYnWY2BjjY3fsB5wO3FSmbyG47+WSo\nqYF/+zf4zndg27bYiURKL29Bd/engPW7OORU4O7ssQuADmamE4RJdAMHhjnqzz4L48fDX/4SO5FI\naRWjh34AsKre7dXZ74lE16kT/P730L07HHssvPlm7EQipaMPRSX19tgDbrsNzjsvFPV582InEimN\n6iI8xmrgwHq3e2S/l9PUqVM/vZ7JZMhkMkWIILJrZvCNb8DnPw+nnQb//u9wzjmxU4nkVlNTQ01N\nTZN/rqCFRWbWG3jY3QfnuO9k4GJ3H2tmI4Cb3H1EI4+jhUUS3SuvwJe/DKecAv/xH1BdjGGNSAkV\nbaWomU0HMkAnYC0wBdgDcHeflj3mVmA0sAk4290XNfJYKuiSCOvXw9/9XSjm990HHTrETiTSOC39\nF8ljy5YwZ33uXHj4YejbN3Yikdy09F8kj9at4cc/hssug5Ej4YknYicSaR4VdGnxLrggbL/7ta+F\n2TAilUotF5Gs114LH5aedBL86Ef6sFSSQy0XkSbq2xeeeSYU9jFjwgenIpVEBV2kng4d4JFHYPBg\nGD48THGU9PvLX+C666BLl7D98qOPQiU2E1TQRRpo1QpuvDFs6nX88TB7duxEUiobNsA118BBB4Uz\nX82dGxagTZ4MQ4bA9OmwdWvslIVTD11kF+bNg9NPh6uugksuCStOpfKtWwc33bTjnLRXXRVWEddx\nD3sA/eAHsGoVXHEF/OM/wp57xsmrHrpIERx/PPzpT3D77WE2zJYtsRNJc7z/PkyaBP36wZo1sGAB\n3HXXZ4s5hBfu0aPhySfhl7+Exx8Po/jrroMPPoiTvRAq6CJ59OkTivqaNfDFL8L//m/sRNJUtbXw\n7W+Hwr1hAyxcGE6CcvDB+X/22GPhoYdgzhx4+eXwM5Mmwdq1pc/dVCroIgXYZx+YOROGDQuXZcti\nJ5JCrF4dFo4NHAh//Wvok//Xf0Hv3k1/rEGD4J574Pnnw4eohxwCF12UrC2ZVdBFCtSqFVx/PUyZ\nApkMPPZY7ETSmHfeCcV28OCwnuDll+GWW6BHj+Y/du/ecOutsGIF7LsvHH00nHkmLF3a/MduLhV0\nkSY666wwWv/61+GGGypzeltavfEGnHsuHH54eFe1YkX4O+revfjP1aULXHtteM4hQ+BLXwo7eM6f\nX/znKpQKushuOPbYsAjpnnvCvuqffBI7Ucv26qthFsqwYdCtW7hdN6+81PbZB/7lX0Jh//KXwwv+\ncceFd3DlfrHXtEWRZti4MfwHfv99ePDB8hSQXNzho49Cb/fDD3e+bN8ORx0V+r5VKRrGLVsWRsmz\nZ4f545deCh07xs20dSs88EB4QXGHK68MJ1VpzlYS2j5XpEy2b4d//Ve4994wG2LwTqeBady2bY0X\n4ca+39h9rVvD3nuHEWPDy/btYYre+vVwzDFhd8mRI8OINtbc6uZYsgS+//0wrfDyy0O/fJ99Yqf6\nLHeYNSvMZV+9esdc9rZtm/5YKugiZTZ9ephRcfnlYRRcSBH++GNo3z53EW6sOOf6/t57h3On5vPe\ne2EK5vz54fLSS+EFqK7AjxwJXbuW/ne1uxYtCis7n3kmTEO84AJo1y52qvzmzw8j9uefh3/+Z7jw\nwqa9AKmgi0Tw3HPwi1/sXKQbK8577RW3BbJ5c8j81FOh6Dz9NHTqFPYzqSvwAwbEb9MsWBAK+eLF\noV997rmV+c5i6dIwU2rWrHDS8ssuK+wFVAVdRJps+/bQl64bwT/1VFgZWdemGTUq9OLLVUyfeioU\n8hUrQi/67LN3r2WRNG++Cf/5n2Ef/jPOCO82+vRp/HgVdBEpivfe21Hg588Pc7oPO+yzbZpifhjs\nHnrjV18Nb70V9lk566zCWkqVZu1auPlmmDYtbDXwne/k/gxGBV1ESmLTpp3bNF26fLbADxjQ9I3M\n3MNuh1dfHQrdd78bziLVunVp/hxJ8sEH4WxZN90U3gFNmhSmxtZRQReRsti+PYza64/iP/ggFKS6\nXvxRRzXeKnEPc7avuSZ8UDx5ctjhslWr8v45kuCjj8JnMD/8YVjVOmlSGLlXVamgi0gka9Z8tsAv\nX76jTTNqVCj2nTqFaZ7XXBPmbk+eDBMnxv8ANgm2boVf/zrMjKmqghdfVEEXkYTYtAmefXZHgX/6\n6bDQplcv+N73YNw4FfJc6t69nHKKCrqIJNS2bWEDrd69ddKQQqiHLiKSEjpjkYhIC6OCLiKSEiro\nIiIpUVBBN7PRZrbCzF41s+/kuP8EM9tgZouyl8nFjyoiIruSt6CbWRVwK/Al4FDgDDMbkOPQee5+\nRPby/SLnLJmamprYEXJKYi5lKowyFS6JuZKYqVCFjNCHASvd/W133wLcB5ya47iKnHyU1L+8JOZS\npsIoU+GSmCuJmQpVSEE/AFhV7/a72e81dIyZLTazR81sYFHSiYhIwZpxUqTPWAj0dPfNZjYGmAn0\nL9Jji4hIAfIuLDKzEcBUdx+dvX0l4O5+/S5+5k3gSHdf1+D7WlUkIrIbCllYVMgI/Tmgr5n1At4D\nvgqcUf8AM+vq7muz14cRXijWNXygQgKJiMjuyVvQ3X2bmV0CzCb03O909+Vmdn6426cBXzGzC4Et\nwEfA6aUMLSIiOyvrXi4iIlI6ZVspmm9xUrmZ2Z1mttbMlsTOUsfMepjZE2b2spktNbNLE5CpjZkt\nMLMXspmmxM5Ux8yqsgvZHoqdpY6ZvWVmL2Z/X8/GzgNgZh3M7Ddmtjz7b2t45Dz9s7+fRdmvHyTk\n3/rlZvaSmS0xs3vNLPpJ78zssuz/u8LqgbuX/EJ44XgN6AW0BhYDA8rx3LvINAoYCiyJmaNBpm7A\n0Oz19sArsX9P2Sx7Zb+2Ap4BhsXOlM1zOfBL4KHYWeplegPYN3aOBpl+AZydvV4N7BM7U71sVcAa\n4MDIOfbP/t3tkb19P3BW5EyHAkuANtn/e7OBg3b1M+UaoRe6OKls3P0pYH3MDA25e627L85e3wgs\nJ/ec/7Jy983Zq20IBSF6n87MegAnA3fEztKAkaA9ksxsH+A4d/85gLtvdfcPI8eq7yTgdXdflffI\n0msFtDOzamAvwgtNTIcAC9z9E3ffBswDJuzqB8r1D6/QxUmSZWa9Ce8gFsRN8mlr4wWgFpjj7s/F\nzgT8CLiCBLy4NODAHDN7zszOjR0G6AP8fzP7ebbFMc3M9owdqp7TgV/FDuHua4AbgHeA1cAGd58b\nNxUvAceZ2b5mthdhAHPgrn4gMSMJ2cHM2gMPAJdlR+pRuft2dz8c6AEMj70S2MzGAmuz72aMZG07\nMdLdjyD857vYzEZFzlMNHAH8JJtrM3Bl3EiBmbUGxgG/SUCWjoSuQS9C+6W9mX0tZiZ3XwFcD8wB\nHgNeALbt6mfKVdBXAz3r3e6R/Z40kH279wBwj7v/Lnae+rJv1f8AjI4cZSQwzszeIIzuTjSzuyNn\nAsDd38t+fR+YQWg3xvQusMrdn8/efoBQ4JNgDLAw+7uK7STgDXdfl21v/BY4NnIm3P3n7n6Uu2eA\nDcCruzq+XAX908VJ2U+OvwokYWZC0kZ3AD8Dlrn7zbGDAJjZ58ysQ/b6nsAXgBUxM7n7Ve7e090P\nIvxbesLdz4qZCcDM9sq+u8LM2gFfJLxtjsbDgr9VZla3Fcf/A5ZFjFTfGSSg3ZL1DjDCzNqamRF+\nT8sjZ8LMOme/9gTGA9N3dXyx9nLZJW9kcVI5nrsxZjYdyACdzOwdYErdB0cRM40EzgSWZnvWDlzl\n7rMixuoO3JXdRrkKuN/dH4uYJ8m6AjOyW1xUA/e6++zImQAuBe7NtjjeAM6OnIdsT/gk4LzYWQDc\n/Vkze4DQ1tiS/TotbioAHjSz/QiZLsr3gbYWFomIpIQ+FBURSQkVdBGRlFBBFxFJCRV0EZGUUEEX\nEUkJFXQRkZRQQRcRSQkVdBGRlPg/P+qIEKaVpLoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fff665fc050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "\n",
    "For other techniques for training model you may go through the following links:\n",
    "\n",
    "* Callback functions: [`ModelCheckpoint`](https://keras.io/callbacks/#modelcheckpoint), [`ReduceLROnPlateau`](https://keras.io/callbacks/#reducelronplateau), and [`EarlyStopping`](https://keras.io/callbacks/#earlystopping) are quite useful\n",
    "* Hyperparameter search using `sklearn`'s [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) function. An example can be found in `kera`'s [example script](https://github.com/fchollet/keras/blob/master/examples/mnist_sklearn_wrapper.py).\n",
    "* Parallelization: [`mxnet`](https://github.com/dmlc/mxnet) or [Spark](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)\n",
    "* An nice overview on available deep learning library out there (more focused on python): [My Top 9 Favorite Python Deep Learning Libraries](http://www.pyimagesearch.com/2016/06/27/my-top-9-favorite-python-deep-learning-libraries/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
